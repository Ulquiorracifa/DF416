

## yolov2



##### 1.batch normalization

归一化卷基层参数，减少过拟合（可去除随机失活步骤）

（2%mAP）

##### 2.高分辨图像预训练

使用ImageNet提前预训练网络

（4%mAP）

##### 3.Anchor Boxes 替代全连接层	修改输入尺寸

使用anchor boxes 提升最后的卷积层的输出分辨率

从448* 448大小改为416\* 416(416 = 13* 32 )	*下采样32倍*

13为奇数，使得box的中心落在一个cell里

​		（yolov1中分成7* 7 个网格，每个网格预测2个边界框，存在7* 7 *2=98个box）

​		（yolov2中采用anchor boxes，13*13大小的特征图，每个cell预设anchor boxes来回归位置及类别）

##### 4.使用聚类分析获得先验框

因对于大尺度的box产生的聚类参数偏差也大，所以使用了iou来聚类

用iou参数作为聚类距离指标：

![yoloEq](C:\Users\fourt\Desktop\yoloEq.png)

选择了5种先验框，初始先验框对网络预测有帮助。（优于手动选择先验框）

##### 5.直接定位预测

使用rpn中的tx和ty（offset）会使得

![1555936928375](C:\Users\fourt\AppData\Roaming\Typora\typora-user-images\1555936928375.png)

anchor box浮动较大（tx 为1或-1将平移一整个box），模型不稳定



因此

![1555936990497](C:\Users\fourt\AppData\Roaming\Typora\typora-user-images\1555936990497.png)

yolov2使用了tx,ty, tw,th和to直接估计偏移，类似yolov1中的定位。(cx,cy为cell左上角距离，pw,ph为box宽高)



如下图：![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20180929112555718?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3cGx3Zg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### 6.Fine-Grained Features

直通层（类似residentNet）把26* 26* 512转成13* 13* 2048和13* 13* 1024一起组成特征

把高分辨率特征联合输出，提高对小物体的识别提取

##### 7.多尺度训练模型

从320* 320 到608* 608 共10组size 每组相差32，每经过10个batch训练就随机更换不同尺度的图片





### 模型

yolo2模型上使用了Darknet-19



![ 1555941792773](C:\Users\fourt\AppData\Roaming\Typora\typora-user-images\1555941792773.png)



decay :0.0005, momentum:0.9,

##### 训练：

类别训练：

一阶段：Imagenet数据集，初始学习率0.1：160个epoch，图像大小224* 224（ 使用数据增加：随机裁剪、旋转、色度亮度调整）

二阶段：图像输入448* 448，继续训练10个epoch，学习率为0.001



三阶段 ：修改网络，为检测模型。

