---
typora-copy-images-to: pic
typora-root-url: pic
---

### 2019cvpr汇总：



##### Anchor-Free

paper:<https://arxiv.org/pdf/1903.00621v1.pdf>

对小物体检测常用FPN（Feature Pyramid Network）在不同尺度上检测物体。fpn融合深层和浅层的特征。

如faster RCNN中的ROI pooling用于分拣特征，Mask RCNN中庸的ROIAlign.



将size不作为分类那些feature map参与检测的参数。

（60x60大小的car和50x50大小的car被分在了不同feature map，50x50和40x40的被分在了同一级feature map，无法判断优劣）



###### 文章提出：

**Feature Selective Anchor-Free Module (FSAF)**

![img](https://github.com/Ulquiorracifa/DF416/blob/master/pic/gYUsOT36vfoqaonDicB29VlmiczTGRQltU7x.png?raw=true)



**Ground-truth和loss的设计**

![img](https://github.com/Ulquiorracifa/DF416/blob/master/pic/gYUsOT36vfoqaonDicB29VlmiczT.png?raw=true)

分支对应的gt是图中白色区域内值为1，表示正样本，黑色区域内值为0，表示负样本，灰色区域是忽略区域不回传梯度。

:<u>不怎么理解</u>，在文中都是自己定义的大小，



FSAF的设计就是为了达到自动选择最佳Feature的目的，最佳Feature是由各个feature level共同决定。

![img](https://github.com/Ulquiorracifa/DF416/blob/master/pic/gYUsOT36vfoqaonDicC.png?raw=true)



paper:https://arxiv.org/pdf/1904.01355.pdf

##### FCOS：Fully Convolutional One-Stage Object Detection

bg:

- 超参数过多，如anchor number、anchor size、anchor ratio等

- 为保证效果，需要大量的anchors，并保证正负样本均衡

  <u>{问题根据难度从小到大排个序：**大数据+分布均衡<大数据+分布不均衡<小数据+数据均衡<小数据+数据不均衡**}</u>（原解决方案：采样、合成、加权、focal loss等）

- 计算大量anchors IOU

fcos ：

像素级的语义分割；



![1561021169126](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1561021169126.png)



初始流程：

rpn网络：

1.进入特征网络提取featuremap->2.对featuremap每个位置生成各规格的anchors->3.对各个anchors进行回归（分类回归[2分类：前景或后景]、位置回归[平移缩放]）

ROI proposal：

提议策略，保留0.7以上roi的区域框为前景样本，都小于0.3的视为背景样本；其余的舍弃（部分情况不存在0.7以上的positive label，则选择与GT框重叠最高的）

faster rcnn：

![1561033474067](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1561033474067.png)

分为分类网络和定位网络（k+1，k类和背景）

loss设计![1561034662930](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1561034662930.png)

回归bbox：

线性回归Y=WX

![1561034889605](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1561034889605.png)











​					

Pyramid Mask Text Detector（只完善样本标签区分等操作）

利用maskR-CNN的分割方法定位文本：

problem：

1. 过简单的监督（和物体分割不同 ，文本往往是四边形框选区域）
2. 标签不准确（大部分背景区域，属样本噪声）
3. 错误传递（用bbox作为语义分割，对于并未全覆盖的例子是脆弱的（bbox倾向于排除外部内容））

#### **baseline**：

##### 数据增强：

1.50%概率水平翻转图片

2.随机resize长宽到640-2560之间

3.从第二步中随机裁剪640*640区域

##### RPN anchor：

在FPN模块中，量化基本的anchor大小、anchor搜索的特征图、anchor的长宽比。

文章采用了（1/4,1/8,1/16,1/32）四个尺度特征图进行搜索4*4的大小的anchor，长宽比为（0.17; 0.44; 1.13; 2.90; 7.46 ）[确定长宽比由分布{0.17+-:5%，7.46+-:95%}得到]

##### OHEM:

使用OHEM检索评估得出难样本，用以更新网络

#### PTMD： 

​		points：

1.通常文本为四边形，这样破坏了形状信息

2.文本框形式转像素不准确（前后景在maskrcnn中较为重要，因为内容落实到像素，会影响loss）<u>***（标签周围做卷积？滤波前后景预测概率图；结果去卷积得区域？）***</u>

3.bbox的定位错误会影响框内物体分割的结果。

##### score定义方法：

![1559979657809](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559979657809.png)

O为区域高斯金字塔中心：

xo = （xA+xB+xC+xD）/4

yo = （yA+yB+yC+yD）/4

P为区域内一点，可有OM,ON组合得到（M,N为A,B,C,D中取得的两点）

![1559979981667](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559979981667.png)

αβ可计算得：

![1559980070115](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559980070115.png)

则定义的scoreP为

![1559980111352](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559980111352.png)

##### 网络的loss：

和maskRCNN类似，定义为

![1559980279190](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559980279190.png)

##### 由预测值推断结果文本框：

预测的值大于0.1的点为positive点，找中心点，定义为金字塔中心点，赋为1，同时获得金字塔的4个侧平面（Ax+By+Cz+D=0,C =1）。

对预测出的点分配给这四个平面，再返回上一步的计算中心点，采用最小二乘法更新4个平面。

↑上述操作直至最小二乘值小于一定阈值（文章设为1e-4）或者到达最大迭代次数（文章：10）

![1559983533703](https://raw.githubusercontent.com/Ulquiorracifa/DF416/master/pic/1559983533703.png)





##### 